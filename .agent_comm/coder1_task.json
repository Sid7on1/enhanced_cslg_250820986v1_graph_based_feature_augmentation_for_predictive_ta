{
  "agent_id": "coder1",
  "task_id": "task_6",
  "files": [
    {
      "name": "setup.py",
      "purpose": "Package installation setup",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.LG_2508.20986v1_Graph_Based_Feature_Augmentation_for_Predictive_Ta",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.LG_2508.20986v1_Graph-Based-Feature-Augmentation-for-Predictive-Ta with content analysis. Detected project type: computer vision (confidence score: 9 matches).",
    "key_algorithms": [
      "Repetitive",
      "Thematic",
      "Specific",
      "Adaptive",
      "Mining",
      "Machine",
      "Downstream",
      "Encing",
      "Newman",
      "Representation"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.LG_2508.20986v1_Graph-Based-Feature-Augmentation-for-Predictive-Ta.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nGraph-Based Feature Augmentation for Predictive Tasks on\nRelational Datasets\nLianpeng Qiao\nqiaolp@bit.edu.cn\nBeijing Institute of TechnologyZiqi Cao\n3120235211@bit.edu.cn\nBeijing Institute of TechnologyKaiyu Feng\nkaiyufeng@outlook.com\nBeijing Institute of Technology\nYe Yuan\nyuan-ye@bit.edu.cn\nBeijing Institute of TechnologyGuoren Wang\nwanggr@bit.edu.cn\nBeijing Institute of Technology\nABSTRACT\nData has become a foundational asset driving innovation across do-\nmains such as finance, healthcare, and e-commerce. In these areas,\npredictive modeling over relational tables is commonly employed,\nwith increasing emphasis on reducing manual effort through au-\ntomated machine learning (AutoML) techniques. This raises an\ninteresting question: can feature augmentation itself be automated\nand identify and utilize task-related relational signals?\nTo address this challenge, we propose an end-to-end automated\nfeature augmentation framework, ReCoGNN , which enhances ini-\ntial datasets using features extracted from multiple relational ta-\nbles to support predictive tasks. ReCoGNN first captures semantic\ndependencies within each table by modeling intra-table attribute\nrelationships, enabling it to partition tables into structured, se-\nmantically coherent segments. It then constructs a heterogeneous\nweighted graph that represents inter-row relationships across all\nsegments. Finally, ReCoGNN leverages message-passing graph neu-\nral networks to propagate information through the graph, guiding\nfeature selection and augmenting the original dataset. Extensive ex-\nperiments conducted on ten real-life and synthetic datasets demon-\nstrate that ReCoGNN consistently outperforms existing methods\non both classification and regression tasks.\n1 INTRODUCTION\nAcross domains such as finance [ 12], healthcare [ 36], and the inter-\nnet [43], relational table have emerged as a fundamental format for\norganizing structured data. To unlock the value embedded in such\ndata, a broad range of predictive modeling tasks have arisen around\nrelational tables. In response, the machine learning community has\nseen rapid progress in AutoML systems, which aim to automate the\nprocess of model selection and hyperparameter tuning. Provided\nwith a dataset and task specification, these systems strive to achieve\nhigh performance with limited human involvement.\nThe effectiveness of the AutoML is greatly influenced by the\nquality of the user-provided raw data, particularly when the raw\ndata features are insufficient. For instance, an event coordinator\nseeking to predict attendance for an upcoming event may find that\nhistorical data from prior, similar events could be beneficial due to\nits availability in databases. However, dependence solely on such\nhistorical data could result in imprecise predictions. This is attrib-\nutable to multiple other determinants that could impact attendance\nnumbers, including the event\u2019s geographical setting, the long-term\nresidential trends of expected participants, their enthusiasm for theevent\u2019s subject matter, and possibly their social connections, which\nmight sway their decision to attend.\nWhile databases conventionally organize all relevant informa-\ntion into relational tables to facilitate straightforward access, deter-\nmining which features are predictive in many contexts remains a\nsignificant challenge. These predictive features are often dispersed\nacross various interconnected tables within the database. A brute-\nforce approach might involve merging all the potentially relevant\ntables into a single, extensive table. However, this method is fraught\nwith problems. Primarily, such a large table would encompass an\nexcessive number of features, leading to substantial demands on\ncomputational resources and increased processing duration. Ide-\nally, extracting only those features that are genuinely beneficial\nwould be adequate. Nevertheless, selecting attributes from tables\nto feed into an AutoML system for optimal predictive performance\nis challenging due to the following reasons:\n\u2022Feature relevance challenges (Selective Dependency) : In ex-\ntensive databases, many attributes might lack direct predictive\nrelevance for the target attribute. Including these irrelevant fea-\ntures can introduce noise, masking significant patterns. Beyond\nirrelevance, some attributes affect the target indirectly through\ncomplex correlations. These hidden dependencies, often sub-\ntle or combinatorial across various attributes, require in-depth\ndomain knowledge or advanced analytical methods to detect.\n\u2022Table relationship challenges (Complex Dependency): Effec-\ntively leveraging information from multiple related tables intro-\nduces another layer of complexity. While the explicitly defined\nprimary key-foreign key relationships offer a primary mecha-\nnism for data integration, the task becomes significantly more\nintricate when relational schema exhibit complex structural\npatterns. Understanding and navigating these complex inter-\ntable relationships is crucial for building accurate predictive\nmodels.\nIn the face of these challenges, several research efforts have\nemerged. One major line of work focuses on selecting appropri-\nate features for the predictive task from the merged single table.\nClassical methods such as Forward Selection and Backward Elim-\nination incrementally add or remove features to optimize model\nperformance. However, they are prone to local optima. Forward\nSelection may overlook weakly correlated yet useful features, while\nBackward Elimination may mistakenly remove informative ones\nin the presence of noise [ 11]. Methods such as [ 26,35] focus on\nefficiency by avoiding unnecessary joins, particularly when foreign\nkeys provide most required data. Nevertheless, these strategies arearXiv:2508.20986v1  [cs.DB]  28 Aug 2025\n\n--- Page 2 ---\nLianpeng Qiao, Ziqi Cao, Kaiyu Feng, Ye Yuan, and Guoren Wang\ninherently cautious and generally do not enhance performance.\nARDA [ 11] utilizes data discovery mechanisms to rank potential\ntables and employs heuristic methods for selecting features. As\nthese rankings do not consider model specifics, their dependability\nis constrained. Additionally, ARDA is limited to single-hop joins,\nhindering its ability to leverage intricate multi-hop connections.\nIn contrast, AutoFeature [ 31] and METAM [ 15] conceptualize aug-\nmentation as a sequential decision-making task, employing rein-\nforcement learning models such as multi-armed bandits and deep\nQ-networks to dynamically explore join paths, thereby enhancing\ndownstream performance. Nonetheless, these approaches typically\nentail expensive join operations and repetitive model assessments\nacross diverse tables. Leva [ 44], while addressing inter-element\nrelationships in relational databases, presupposes that the impor-\ntance levels of adjacent elements are constant and not amenable to\nadaptive learning.\nRecent approaches have explored transforming databases into\ngraph structures capable of naturally representing intricate rela-\ntionships between tables and efficiently managing multihop join\npaths. The methods proposed by [ 13] and [ 14] involve mapping\neach table tuple to a graph node and employing primary key-foreign\nkey (PK-FK) relationships as graph edges. However, this graph con-\nstruction technique presumes that all features of the entire table\nare viewed collectively, neglecting the interaction relationships\nbetween individual features.\nIn this study, we present an innovative end-to-end automated\nframework named ReCoGNN .ReCoGNN serves as a tool that en-\nhances initial datasets with features extracted from various rela-\ntional tables, facilitating predictive tasks. The framework operates\nthrough three primary phases: Initially, it examines the associations\nbetween attributes across all tables (auxiliary tables) that can be\njoined or indirectly related to the initial data (base table) to discern\nsemantic dependencies. This involves identifying attribute clusters,\nthereby partitioning the auxiliary tables into segments that are\nstructurally and thematically refined. Secondly, using the segments\nderived from all auxiliary tables along with the fundamental data,\na weighted heterogeneous graph model is constructed. This model\ncharacterizes the relational network of records in different table\nsubsets. Lastly, the protocol used is the message passing mechanism\nof Graph Neural Networks (GNN), which facilitates the dissemi-\nnation and aggregation of information within the graph structure.\nThis mechanism is used to filter the attributes significant for fea-\nture augmentation using learning edge weights, thus enhancing the\nfeature set of the initial data to improve the predictive performance.\ndownstream.\nUnlike table-based methods such as [ 11,26,35], the ReCoGNN\nframework provides specific advantages. (I) The utilization of a\ngraph structure enables the depiction of diverse entities and their\ninteractions via nodes and edges, resulting in a clear and concise rep-\nresentation of one-to-many, nested, and hierarchical relationships.\nThis method significantly diminishes the complexity associated\nwith join operations found in traditional systems, thus facilitating\na more lucid and effective visualization of relationships. (II) The\ngraph-based approach efficiently addresses the problem of data re-\ndundancy and inflation typical in conventional techniques. Unlike\ntraditional strategies that require the transformation of all featuresinto a high-dimensional matrix, graph approaches necessitate only\nthe storage of nodes and their interconnections.\nIn contrast to other graph-based methodologies, the ReCoGNN\nframework presents distinct advantages by facilitating detailed fea-\nture enhancement via an analysis of attribute associations among\ntables and the identification of attribute groups, which results in\nmore detailed thematic segmentation. It builds a weighted heteroge-\nneous graph model that represents complex relationship networks\nthrough advanced weighting and heterogeneity techniques, surpass-\ning basic join operations to improve the robustness and precision\nof node embeddings. Furthermore, ReCoGNN employs GNN to es-\ntablish a robust information propagation mechanism, wherein the\nlearning and optimization of edge weights aid in noise filtration\nand the selection of significant attributes for augmenting features.\nContributions. In summary, the contributions of this paper are:\n\u2022We propose ReCoGNN , an automated feature enrichment frame-\nwork that leverages GNNs over relational data graphs to im-\nprove prediction tasks with minimal manual effort.\n\u2022In parsing attribute associations across tables, ReCoGNN iden-\ntifies attribute groups, splitting the original tables into more\nrefined thematic subsets, allowing the model to more accurately\ncapture semantic dependencies within the data.\n\u2022We conducted extensive empirical research using multiple real-\nworld and virtual relational datasets to evaluate our proposed\nmethod. The experimental results strongly validated the effec-\ntiveness and practical value of the ReCoGNN framework.\n2 PROBLEM FORMULATION\nPreliminary. Initially, we establish and clarify the essential con-\ncepts utilized across this paper.\n\u2022Base Table : The base table, denoted as \ud835\udc470={\ud835\udc4e1,\ud835\udc4e2,...,\ud835\udc4e\ud835\udc5b,\ud835\udc47},\nis a unique table that stores the initial data without any aug-\nmentation. Each \ud835\udc4e\ud835\udc56(\ud835\udc56\u2208[1,\ud835\udc5b])represents a feature attribute\nand\ud835\udc47denotes the target attribute.\n\u2022Auxiliary Tables : The auxiliary tables {\ud835\udc471,\ud835\udc472,...,\ud835\udc47\ud835\udc3e}are\na set of tables that could join the base table either directly\nor indirectly through the other auxiliary table. Each auxiliary\ntable\ud835\udc47\ud835\udc58=\ud835\udc4e\ud835\udc58\n1,\ud835\udc4e\ud835\udc58\n2,...,\ud835\udc4e\ud835\udc58\ud835\udc5b, where each \ud835\udc4e\ud835\udc58\n\ud835\udc56(\ud835\udc56\u2208[\ud835\udc56,\ud835\udc5b])represents an\nattribute within the table \ud835\udc47\ud835\udc58.\nIn a relational table, each tuple represents an entity or rela-\ntionship instance, with attributes assigned to specific data types\n(e.g., numerical, string, or categorical). Tables are interconnected\nthrough keys: primary keys uniquely identify tuples, and foreign\nkeys reference primary keys to enforce referential integrity.\nPredictive tasks on relational tables learn mappings from input\nattributes to a target attribute. For instance, as shown in Figure 1 the\ninterest attribute in Event-Interest is the target attribute, i.e., the\nrecommendation system wishes to forecast the event of interest for\neach user.\ud835\udc470is the base table, while \ud835\udc471,...,\ud835\udc47 4are auxiliary tables.\nThese tasks are categorized as classification (categorical target) or\nregression (numerical target). For example, predicting the status of\na house sale is a classification task, whereas estimating its price is\na regression.\nOur objective is to predict user interest in specific events. In\ntheEvent-Attendees (T4), the invited andyesattributes represent\n\n--- Page 3 ---\nGraph-Based Feature Augmentation for Predictive Tasks on Relational Datasets\nUsers\nuser\t(PK)\nint\nlocale\nvarchar\nbirthyear\nint\ngender\nvarchar\njoinedAt\nvarchar\nlocation\nvarchar\ntimezone\nfloat\nEvent-Interest\nid\t(PK)\nint\nuser\t(FK)\t\nint\nevent\t(FK)\t\nint\ninvite\nvarchar\ntimestamp\ndatetime\ninterested\n\t\nint\nEvents\nevent\t(PK)\nint\nuser\t(FK)\nint\nstart_time\ndatetime\ncity\nvarchar\t\nstate\nvarchar\nzip\nvarchar\ncountry\nvarchar\nlat\nfloat\t\nlng\nfloat\nc_1_to_c_100\nvarchar\nc_other\nvarchar\nUser-Friends\nid\t(PK)\nint\nuser(FK)\nint\nfriends\nvarchar\nEvent-Attendees\nid\t(PK)\t\nint\nevent\t(FK)\nint\nyes\nvarchar\nmaybe\nvarchar\ninvite\nvarchar\nno\nvarchar\nT\n0\nT\n1\nT\n2\nT\n3\nT\n4\nFigure 1: Event-Recommendation Relational Tables\ninvitation status and confirmed attendance respectively. These in-\ndicators reflect event popularity and may influence user interest.\nSuch attributes can correlate with our prediction target, providing\nvaluable auxiliary signals. Identifying attribute correlations across\nauxiliary tables enhances predictive performance. However, detect-\ning these relationships in complex database structures presents\nchallenges. While some attributes are independently predictive,\nothers gain value only in combination. Feature interactions reveal\npatterns invisible when examining features in isolation, requiring\ncombinatorial analysis. For example, in Events (T3), the latand\nlngattributes individually offer limited information but jointly de-\ntermine event location - a factor likely affecting interest. These\ncombinations often embody complex semantic relationships that\nare difficult to manually define or encode. Such challenges motivate\nthe development of an end-to-end learning framework capable of\nautonomously discovering synergistic attribute combinations. By\nenriching base table features through this process, we can improve\nmodel performance while minimizing manual feature engineering\nefforts.\nProblem Statement. Given a base table \ud835\udc470with the target attribute\nand auxiliary tables {\ud835\udc471,...,\ud835\udc47\ud835\udc3e}, we aim to augment \ud835\udc470with at-\ntribute from{\ud835\udc47\ud835\udc58}to improve prediction accuracy for the target.\nHowever, the core challenge lies in identifying relevant attributes\nfrom{\ud835\udc47\ud835\udc58}whose integration into \ud835\udc470enhances model performance.\n3 SOLUTION OVERVIEW\nAs previously discussed, integrating discriminative features from\nauxiliary tables into the base table poses two key challenges: as-\nsessing feature importance and modeling inter-table relationships.\nWe propose ReCoGNN , a comprehensive framework (illustrated in\nFigure 2) to address these challenges. In the first phase, ReCoGNN\nuses a graph neural network to identify task-relevant links among\ntable attributes, segmenting the original tables into semantically\naligned sub-tables. In the second phase, ReCoGNN forms a hetero-\ngeneous graph incorporating both explicit inter-table connections\nand implicit similarity ties. A graph neural network is employed to\nrefine the tuple representations in the base table, thereby improving\nfeatures for the predictive task.Stage One. In the first stage of ReCoGNN , called \"Task-Relevant\nTable Structuring,\" our goal is to extract from each original table a\nset of sub-tables composed of highly related (and potentially over-\nlapping) attributes. This approach moves away from the traditional\npractice of treating a table as a monolithic structure, instead gener-\nating semantically coherent attribute groups that are more directly\naligned with the prediction task. Intuitively, not all tuples are useful\nfor the task, as many cannot be directly or indirectly joined with\nthe base table, rendering them uninformative for prediction. There-\nfore, we first identify the most task-relevant tuples in each table\nbased on their potential contribution to the prediction objective.\nBy focusing subsequent analysis on this informative subset, we\nenhance both the efficiency and effectiveness of feature modeling\nand downstream learning.\nStage Two. In the second stage of ReCoGNN , we aim to enhance\nthe feature representation of the base table by integrating infor-\nmation from the sub-tables generated in the first stage. Intuitively,\nnot all sub-tables contribute equally to the prediction task. Thus,\nthe key challenge at this stage is to select the most task-relevant\nsub-tables and effectively integrate them into the base table. More-\nover, since it is difficult to precisely evaluate whether the intra-table\nrelationships identified in the first stage fully align with the pre-\ndiction task requirements, we assess the effectiveness of feature\nenhancement based on the final prediction performance in the sec-\nond stage. Therefore, we design a heterogeneous graph and employ\na Graph Neural Network (GNN) to learn edge weights, thereby\nidentifying and filtering attributes that are meaningful for feature\nenhancement. While achieving this objective, the GNN also aggre-\ngates relevant information from associated sub-tables to the base\ntable nodes, ultimately generating highly valuable representations\nfor downstream prediction tasks. The learned representations can\nbe directly applied to different prediction heads (e.g., classifiers or\nregressors), supporting diverse downstream applications.\n4 TASK-RELEVANT TABLE STRUCTURING\nTo enhance the predictive performance of the base table, we aim to\nincorporate useful features from auxiliary tables in the database.\nConstructing effective attribute sets from auxiliary tables is non-\ntrivial, as individual attributes may not be discriminative alone but\nbecome useful in combination, while the tables themselves contain\nheterogeneous data types.To tackle these obstacles, we introduce\na method for modeling intra-table attribute relationships within\neach supplementary table. Each tuple is viewed as an instance of\nattribute co-occurrence, enabling the creation of a fully connected\ngraph among attributes. A Graph Attention Network (GAT) is then\nemployed to learn the interaction strengths. Task-relevant attribute\ncombinations are identified through learned edge weights and used\nto form more focused, informative sub-tables. The methodology\ninvolves three steps: (1) pinpointing task-relevant tuples for base\ntable integration to gain supervision; (2) constructing attribute-\nlevel graphs for these tuples and training a GAT to capture feature\ninteractions; and (3) deriving highly relevant attribute groups from\nattention weights to reorganize each supplementary table into a\ncompact, task-specific sub-table.\n\n--- Page 4 ---\nLianpeng Qiao, Ziqi Cao, Kaiyu Feng, Ye Yuan, and Guoren Wang\nx\n1\nx\n2\n...\nx\nn\n3\n3\n2\n1\n3\n1\n3\n1\n1\n2\n1\n2\nT\n1_2\nT\n1_1\nT\n1_2\nT\n1_1\nT\n2_1\nT\n2_2\nT\n2_2\nT\n2_2\nT\n2_1\nT\n2_1\nC1\nAugmenting\tData\nBase\tand\tNew\tAuxiliary\tTables\nT\n0\t\nInitial\tEmbeddings\nUser\n2\n1\nT\n1_1\nT\n1_2\nT\n2_2\nC\n2\nC\n3\nAuxiliary\tTables\nProduct\n1\n2\n3\nTrade\n2\n3\n1\nT\n1\nT\n2\nT\n2_1\nC\n1\nC\n1\nC\n2\nC\n3\nC\n4\nx\n1\nx\n2\n...\nx\nn\ny\ni\nPredict\n2\n1\nC1\nT\n1_1\nT\n2_1\nT\n2_2\nT\n1_2\nC\n1 \n1\n3\n1\n3\n1\n2\n3\n1\n2\n3\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nENCODER\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nx\n1\nx\n2\n...\nx\nn\nInter-Table\tGraph\nTop-k\n4\n3\n...\n...\n...\n...\n...\n5\n6\n2\n4\n5\nModeling\tAttribute\tRelationship\n1\nC\n1\nC\n2\nC\n3\nC\n4\n3\nC\n1\nC\n2\nC\n3\nC\n1\nC\n2\nC\n3\nC\n4\nL\nT\n1\n1\n3\n2\nT\n2\nC\n1\nC\n2\nC\n3\nC\n4\nC\n2\nC\n3\nC\n1\nC\n2\nC\n3\nL\nL\nT\n0\nT\n1\nT\n0\nT\n2\nTask-Relevant\nTuples\nT\n1\nT\n2\nG\n1\nTuples'\tGraphs\nGAT\nG\n1\nG\n2\nG\n1\nG\n3\nG\n1\nG\n3\nGAT\nA\nsum\nA\n1\nA\n2\nA\n1\nA\n3\nLearned\tEdge\tweights\nT\n1\nA\nsum\nT\n2\nC\n1\nC\n3\nC\n2\nC\n4\nC\n1\nC\n3\nC\n2\nSub-tables\u2019\tSchema\nGroup\tAttributes\tinto\tSub-tables\nTask-Relevant\tTable\tStructuring\nGraph-based\tData\tAugmentation\nG\n2\n...\n...\nExplicit\tEdges\nImplicit\tEdges\nFigure 2: ReCoGNN Framework\n4.1 Identifying Task-Relevant Tuples\nRecall that our goal is to uncover meaningful combinations of\nattributes within each table that contribute to predicting the tar-\nget attribute\u2019s value. To achieve this, we analyze the relationships\nbetween attribute values within individual tuples of the table. Iden-\ntifying task-relevant tuples is non-trivial. The natural intuition\nsuggests that a tuple is task-relevant if it can be linked to a target\nattribute. However, given the complex schema relationships, a sin-\ngle tuple may connect to multiple target values. So the key question\nto be answered is: Which tuples are task-relevant and useful for\nlearning attribute relationships?\nIntuitively, a tuple in an auxiliary table is considered potentially\nrelevant to a target attribute value if it can be linked to a tuple in the\nbase table. This linkage is established through a sequence of join\noperations defined by the relational schema. However, relational\nschema can be complex, offering numerous join paths. Many of such\njoin paths may not capture semantically meaningful relationships\nfor our specific prediction task. To address this, we construct meta-\npath for each auxiliary table to define its semantic relationship to\nthe base table. As described earlier, the join relationships between\nthe base table \ud835\udc470and all auxiliary tables {\ud835\udc471,...,\ud835\udc47\ud835\udc3e}. are defined by\nthe database schema. This relationship can be effectively modeled\nas a Directed Join Graph (DJG), wherein each node represents a\nrelational table within the dataset \ud835\udc47={\ud835\udc470}\u222a{\ud835\udc471,...,\ud835\udc47\ud835\udc3e}, while the\nedges signify executable join operations in \ud835\udc47, incorporating their\nrespective link types such as 1 : 1 ,1 :\ud835\udc5b, and\ud835\udc5b: 1. However, an\nunprocessed DJG presents complex pathways, which complicates\nthe handling of join relationships, particularly those involving cir-\ncular structures. To effectively identify a meaningful meta-path for\neach auxiliary table, we employ a greedy path search algorithmstarting from the base table \ud835\udc470, expanding the path one step at a\ntime. At each step, the algorithm chooses the next join that leads to\nthe highest path score. The path scoring function jointly considers\ntwo aspects:\n\u2022Path length: Shorter paths are generally more interpretable\nand less likely to introduce redundant or noisy features. The\npath length score is defined as \ud835\udc46\ud835\udc3f=1\n1+\ud835\udc3f, where\ud835\udc3fis the number\nof edges in the current path.\n\u2022Join directionality: One-to-many (1:n) joins are prone to am-\nplifying noise and redundancy, especially when they appear re-\npeatedly in multi-hop paths. To model this, we define a weighted\npenalty term that accounts for the risk associated with each 1:n\njoin:\n\ud835\udc46\ud835\udc41=1\n1+\u00cd\ud835\udc3f\n\ud835\udc56=1\ud835\udc64\ud835\udc56\u00b7I1:n(\ud835\udc52\ud835\udc56), where\ud835\udc52\ud835\udc56denotes the \ud835\udc56-th edge in the\npath,I1:n(\ud835\udc52\ud835\udc56)is an indicator function that equals 1 if the edge\nis a 1:n join, and \ud835\udc64\ud835\udc56is a weight reflecting the risk (e.g., average\nfan-out) of the join.\nThe overall path score is computed as a weighted sum: \ud835\udc46path=\n\ud835\udefc\ud835\udc46\ud835\udc3f+\ud835\udefd\ud835\udc46\ud835\udc41where\ud835\udefcand\ud835\udefdare tunable weights that balance structural\nsimplicity and join stability. Other domain knowledge or schema-\nlevel constraints can also be incorporated into this scoring function.\nDuring greedy search, the algorithm expands only the neighbor\nwith the highest incremental path score at each step, avoiding the\ncost of exhaustive enumeration. Once the auxiliary table is reached,\nthe path with the highest accumulated score is selected as its meta-\npath.\nGiven an auxiliary table \ud835\udc47\ud835\udc56={\ud835\udc61\ud835\udc561,...,\ud835\udc61\ud835\udc56\ud835\udc5b}and its specified meta-\npath, if there exists an instance of a join path that successfully links\na tuple\ud835\udc61\ud835\udc56\ud835\udc57in\ud835\udc47\ud835\udc56to a tuple\ud835\udc610\ud835\udc58in the base table, we consider tuple\n\n--- Page 5 ---\nGraph-Based Feature Augmentation for Predictive Tasks on Relational Datasets\nEvent_Interest\nid(PK)\nuser\nevent\n...\ninterested\n1\n1\n2\n...\n1\n2\n2\n3\n...\n1\n3\n2\n2\n...\n0\n4\n2\n3\n...\n1\n5\n5\n2\n...\n0\nUsers\nuser(PK)\n...\n1\n...\n2\n...\n3\n...\n4\n...\n5\n...\nBase\tTable\nAuxiliary\tTable\n\t\nUser-Friends\nid(PK)\nuser(PK)\n...\n1\n1\n...\n2\n2\n...\n3\n3\n...\n4\n4\n...\n5\n5\n...\nMeta\tPath\t:\nT\n0\nT\n1\nT\n2\nT\n1\nT\n2\nuser\nlable\nlable:1\nlable:0\nlable:1\nlable:0\nlable:1,0,1\nT\n0\nuser\nFigure 3: An Example of Join Instance\n\ud835\udc61\ud835\udc56\ud835\udc57relevant to tuple \ud835\udc610\ud835\udc58and thus carrying the corresponding target\nlabel. Consider the example in Figure 3. As shown in the figure, the\nbase table is Event-Interest , with the target attribute interest and\nUsers serving as an auxiliary table. The tuple with the primary key\n\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f=1in the User table obtains the value of the label from the\nBase table by linking to the tuple in the Event-interest table where\nthe foreign key \ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc5f=4.\nCoreset. In real-world scenarios, database tables can contain a\nvast number of tuples. Utilizing every tuple ensures information\ncompleteness but results in extremely high computational costs for\nfurther processing. To mitigate this, we employ a sampling strategy:\nforming a coreset by selecting a representative subset of tuples from\neach auxiliary table to approximate the entire dataset. This coreset\nmaintains essential structural details while significantly lowering\ncomputational costs, albeit with a slight potential compromise in\nmodel accuracy. Our approach initiates coreset construction from\nthe base table, selecting core tuples based on criteria such as label\ndistribution. These core tuples are then extended to auxiliary tables\nalong meta-paths.\n4.2 Modeling Attribute Relationship with GAT\nBuilding upon the task-relevant tuples identified in Section 4.1,\nwe now turn our attention to discerning the predictive power of\nindividual attributes within these auxiliary tables. However, it is\nnon-trivial task, as the contribution of each attribute to the target\nprediction task is not equal. Some might be directly indicative of the\ntarget, while others might be noisy or only relevant in conjunction\nwith other attributes. To effectively capture these complex inter-\nattribute dependencies and automatically learn their relevance to\nthe prediction task, we propose to represent each task-relevant\ntuple as a complete graph.\n4.2.1 Construct a Complete Graph for Each Tuple. For each identi-\nfied task-relevant tuple in an auxiliary table, we denote its non-key\nattributes as{\ud835\udc501,\ud835\udc502,...,\ud835\udc50\ud835\udc41}. Here we explicitly exclude the pri-\nmary key of the auxiliary table itself and any foreign keys it might\ncontain, as their inherent values primarily serve for identification\nand inter-table linkage rather than directly representing features\nrelevant to the prediction task within the scope of this table\u2019s at-\ntributes. For such tuple \ud835\udc61, we construct an individual complete\ngraph\ud835\udc3a\ud835\udc61=(V\ud835\udc61,E\ud835\udc61,X\ud835\udc61). The set of nodes in \ud835\udc49\ud835\udc61corresponds to the\nattribute values in {\ud835\udc501,\ud835\udc502,...,\ud835\udc50\ud835\udc41}within the tuple \ud835\udc61. Given the\n\u2026\nTask-Relevant\tTuples\nGAT\nPredict\nPrediction\tHead\nt\n1\n:\nt\n2\n:\nt\nm\n:\nG\n1\nG\n2\nG\nm\ng\n1\n\nG\n1\nG\n2\nG\nm\ng\n2\ng\nm\ny\n1\ny\n2\n...\ny\nm\nc\n11\nc\n12\nc\n13\nc\n14\nc\n21\nc\n22\nc\n23\nc\n24\nc\nm\n1\nc\nm\n2\nc\n23\nc\n24\nA\n1\nA\n2\nA\n3\n\u2026Figure 4: Modeling Attribute Relationship with GAT\nheterogeneous data types of the attribute values (numerical, cate-\ngorical, and text), the feature vector X\ud835\udc61[\ud835\udc56]for node\ud835\udc63\ud835\udc56is encoded\naccording to the data type of the corresponding attribute as follows:\nNumerical : For all task-relevant tuples, identify the data type by\ncolumn. For each element \ud835\udc50\ud835\udc56of the identified numeric type columns\n(from any tuple \ud835\udc61), use a model[ 16] with shared parameters to\nencode it to obtain the encoded result e\ud835\udc56\u2208R\ud835\udc51num.\nCategorical : For each cell \ud835\udc50\ud835\udc56recognized as belonging to a cate-\ngorical column, we employ an encoding mechanism utilizing an\nembedding look-up based encoder to transform it into a vector\ne\ud835\udc56\u2208R\ud835\udc51cat.\nText : Regarding text columns, there are several impressive models\navailable, such as Sentence-BERT [ 32]. Encode the text category \ud835\udc50\ud835\udc56\ninto a vector e\ud835\udc56\u2208R\ud835\udc51text.\nFor each cell \ud835\udc50\ud835\udc56, generate embeddings based on its data type:\ne\ud835\udc56=\uf8f1\uf8f4\uf8f4\uf8f4 \uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\ud835\udc53num(\ud835\udc50\ud835\udc56)\u2208R\ud835\udc51num,if type(\ud835\udc50\ud835\udc56)=num\n\ud835\udc53cat(\ud835\udc50\ud835\udc56)\u2208R\ud835\udc51cat, if type(\ud835\udc50\ud835\udc56)=cat\n\ud835\udc53text(\ud835\udc50\ud835\udc56)\u2208R\ud835\udc51text,if type(\ud835\udc50\ud835\udc56)=text(1)\nwhere\ud835\udc53num,\ud835\udc53catand\ud835\udc53textare the numerical encoder, the categorical\nencoder, and the text encoder with output dimensions \ud835\udc51num,\ud835\udc51cat,\nand\ud835\udc51text, respectively. In the experiment, we use PyTorch Frame,\nan advanced modular deep learning extension for PyTorch [21].\nEmbedding dimensions generated by encoders for different cell\ncategories (e.g., numerical, categorical, text) often exhibit signif-\nicant inconsistencies. Specifically, text-type cells typically yield\nhigher-dimensional embeddings compared to their numerical and\ncategorical counterparts. To address this heterogeneity, deep learn-\ning methods such as Multi-Head Self-Attention mechanisms [ 37]\nor adapted ResNet [ 17] architectures can be employed to project\ncolumn-wise embeddings into a unified dimensional space, thereby\nenabling seamless integration for downstream tasks.\nProject embeddings of varying dimensions into a unified target\ndimension\ud835\udc51out:\nh\ud835\udc57=e\ud835\udc57W\u2208R\ud835\udc51\ud835\udc5c\ud835\udc62\ud835\udc61 (2)\nwhere Ware learnable projection matrices for dimensionality align-\nment to\ud835\udc51out.\n4.2.2 Leveraging GAT for Attribute Relationship. Having con-\nstructed a complete graph for each task-relevant tuple, where nodes\nrepresent attributes and edges signify potential relationships, the\nnext crucial step is to capture the intricate and task-specific rela-\ntionships between attributes within each tuple. The general idea is\nshown in Figure 4. To achieve this, we first build a complete graph\n\n--- Page 6 ---\nLianpeng Qiao, Ziqi Cao, Kaiyu Feng, Ye Yuan, and Guoren Wang\nfor each identified task-relevant tuple. These complete graphs are\nencoded by a share-parameter GAT. The representation of each\ngraph is fed into a prediction head, aiming to predict the label asso-\nciated with the corresponding tuple. Unlike methods that treat all\nnodes uniformly, GATs employ an attention mechanism that allows\neach node to dynamically learn the importance of its connection\nwith other nodes in the graph in the context of the target prediction\ntask. The learned attention weights on the edges of the complete\ngraphs serve as indicators of the significance of these pairwise node\ninteractions (representing attribute relationships) for predicting the\ntarget attribute label.\nGiven the complete graph \ud835\udc3a\ud835\udc56(\ud835\udc49,\ud835\udc38,\ud835\udc4b\ud835\udc56)constructed from the \ud835\udc56-th\ntask-relevant tuple, let \ud835\udc62and\ud835\udc63be two nodes in the graph. The\nattention weight \ud835\udefc\ud835\udc56\ud835\udc62\ud835\udc63is derived as follows:\nh\ud835\udc56\n\ud835\udc62=WX\ud835\udc56[\ud835\udc62],Wh\ud835\udc63=X\ud835\udc56[\ud835\udc63],\n\ud835\udc52\ud835\udc56\n\ud835\udc62\ud835\udc63=LeakyReLU\u0010\n\ud835\udefc\u22a4\u0002\nh\ud835\udc56\n\ud835\udc62\u2225h\ud835\udc56\n\ud835\udc63\u0003\u0011\n,\n\ud835\udefc\ud835\udc56\n\ud835\udc62\ud835\udc63=exp(\ud835\udc52\ud835\udc56\ud835\udc62\ud835\udc63)\u00cd\n\ud835\udc64\u2208\ud835\udc49exp(\ud835\udc52\ud835\udc56\ud835\udc62\ud835\udc64)(3)\nwhere\ud835\udc4b\ud835\udc56[\ud835\udc62]and\ud835\udc4b\ud835\udc56[\ud835\udc63]are feature vectors for \ud835\udc62and\ud835\udc63. The\nattention mechanism \ud835\udefcand weight matrix \ud835\udc4aof the GAT layer are\nshared across all complete graphs. The updated embedding \u210e\ud835\udc56\ud835\udc63\u2032\nfor node\ud835\udc63is computed by aggregating the feature vectors of its\nneighbors, weighted by the learned attention coefficient\nh\ud835\udc56\n\ud835\udc63\u2032=\ud835\udf0e(\u2211\ufe01\n\ud835\udc62\ud835\udefc\ud835\udc56\n\ud835\udc62\ud835\udc63W\u2032h\ud835\udc56\n\ud835\udc62) (4)\nwhere\ud835\udc62is any other node in the complete graph, \u210e\ud835\udc62is the embed-\nding of\ud835\udc62,\ud835\udc4a\u2032is a learnable weight matrix, and \ud835\udf0eis a non-linear\nactivation function.\nThe graph-level representation gof the complete graph \ud835\udc3a\ud835\udc56is\nderived by applying a pooling function to the set of its learned node\nembeddings{h\ud835\udc56\ud835\udc63}\ud835\udc63\u2208\ud835\udc49as follows:\ng\ud835\udc56=POOL(h\ud835\udc56\n\ud835\udc63|\ud835\udc63\u2208\ud835\udc49) (5)\nThe resulting graph-level representation \ud835\udc54\ud835\udc56, which encapsulates\nthe learned relationships between attributes within the correspond-\ning task-relevant tuple, is fed into a prediction head. The architec-\nture of this prediction head is specifically tailored to the type of the\ntarget attribute in the base table. The specific architecture of the\nprediction head is orthogonal to our approach. Any standard net-\nwork suitable for the target task (classification or regression) can be\nemployed. For a regression task, the prediction head outputs a con-\ntinuous value \u02c6\ud835\udc66\ud835\udc56, and the lossL\ud835\udc56is computed using Mean Squared\nError (MSE) between \u02c6\ud835\udc66\ud835\udc56and the true target attribute value \ud835\udc66\ud835\udc56. In\ncontrast, if it is a classification task, the prediction head outputs a\nprobability distribution over the possible classes \u02c6\ud835\udc5d\ud835\udc56, and the lossL\ud835\udc56\nis computed using cross entropy loss between \u02c6\ud835\udc5d\ud835\udc56and the one-hot\ntrue class label \ud835\udc66\ud835\udc56. The total loss is computed by aggregating the\nindividual prediction losses as follows:\nL\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59=\ud835\udc35\u2211\ufe01\n\ud835\udc56=1L\ud835\udc56\nThe GAT mechansim learns attention weights for every edge in\neach complete graph. These learned weights are derived directly\nduring the optimization process aimed at minimizing the predictionloss. Thus, they provide a valuable insight into the contribution of\neach attribute pair to the final prediction. Higher weights signify\nmore influential attribute relationships for the predictive task. These\nlearned attribute relationships will be leveraged to group attributes\ninto more focused sub-tables.\n4.3 Group Attributes into Sub-tables\nHaving modeled the pairwise relationships between attributes\nwithin each task-relevant tuple using GAT and obtained edge\nweights reflecting their predictive significance, we now aim to\nleverage this information to structure the original auxiliary tables\ninto more focused, task-relevant sub-tables. To begin with, we first\nintroduce how to extract significant attribute relationships based\non the edge weights learned from all task-relevant tuples.\n4.3.1 Extracting Significant Attribute Relations. For each task-\nrelevant tuple \ud835\udc61\ud835\udc56, we use A\ud835\udc56to denote the learned edge weights,\nwhere each entry A\ud835\udc56[\ud835\udc62][\ud835\udc63]=\ud835\udefc\ud835\udc56\ud835\udc62,\ud835\udc63, the attention weight between \ud835\udc62\nand\ud835\udc63in graph\ud835\udc54\ud835\udc56. To consolidate the importance of these pairwise\nrelationships across all task-relevant tuples, we compute a cumula-\ntive attribute relationship matrix A\ud835\udc60\ud835\udc62\ud835\udc5aby summing the individual\nweight matrices:\nA\ud835\udc60\ud835\udc62\ud835\udc5a=\ud835\udc35\u2211\ufe01\n\ud835\udc56=0A\ud835\udc56 (6)\nTo ensure a consistent scale for identifying significant relation-\nships, we then normalize the values in A\ud835\udc60\ud835\udc62\ud835\udc5a to the range[0,1]\nusing min-max normalization. This normalized matrix A\ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a re-\nflects the overall importance of each pairwise attribute relationship\nacross the entire set of task-relevant tuples. We then identify the\nmost important attribute relationships by applying a predefined\nthreshold\u2113. An edge between node \ud835\udc62and\ud835\udc63is considered significant\nif its corresponding weight A\ud835\udc5b\ud835\udc5c\ud835\udc5f\ud835\udc5a[\ud835\udc62][\ud835\udc63]is greater than \u2113. We use\n\ud835\udc38\ud835\udc60\ud835\udc56\ud835\udc54to denote the set of selected edges. These selected edges rep-\nresent the attribute pairs that the model has consistently deemed\nmost relevant for predicting the target variable across all considered\ncontexts.\nIdentifying significant pairwise attribute relationships provides\nvaluable insights, yet directly utilizing the original wide table for\ndownstream tasks can still be suboptimal. A key challenge lies in the\nfact that not all identified relationships might be equally relevant\nin all contexts, and the presence of weakly related or irrelevant\nattributes within the same table can introduce noise and dilute the\nmore potent signals. Therefore, organizing the attributes into more\nfocused sub-tables, based on their learned relationships, holds the\npotential to create more semantically coherent and task-relevant\ndata segments.\n4.3.2 Extracting sub-tables. Motivated by the potential benefits\nof focusing on strongly related attributes, we now introduce our\nstrategy for extracting more targeted sub-tables from the original\nauxiliary table. To achieve this extraction, we construct a graph\n\ud835\udc3a\ud835\udc60\ud835\udc56\ud835\udc54(V,E\ud835\udc60\ud835\udc56\ud835\udc54)to explicitly model the relationship between attributes\nbased on the set E\ud835\udc60\ud835\udc56\ud835\udc54of selected significant edges. By analyzing the\nstructure of this graph formed by gathering all significant edges,\nwe aim to discover tightly knit groups of highly relevant attributes.\nSpecifically, we propose to identify the set of maximal cliques\n{\ud835\udc361,\ud835\udc362,...,\ud835\udc36\ud835\udc61}within\ud835\udc3a\ud835\udc60\ud835\udc56\ud835\udc54. Let be the set of identified maximal\n\n--- Page 7 ---\nGraph-Based Feature Augmentation for Predictive Tasks on Relational Datasets\nC\n1\nC\n3\nC\n2\nC\n4\nT\n1_1\nT\n1_2\nT\n1_1\nT\n1_2\nT\n1\nC\n1\nC\n2\nC\n3\nC\n4\nSub-tables\u2019\t\tSchema\nC\n1\nC\n3\nC\n2\nC\n4\nOriginal\t\tSchema\nSub-tables\u2019\t\tSchema\nT\n1\nOriginal\t\tSchema\nDisjointness\nOverlap\nC\n1\nC\n2\nC\n3\nC\n4\nC\n1\nC\n2\nC\n3\nC\n4\nC\n1\nC\n2\nC\n3\nC\n2\nC\n4\nFigure 5: Scenarios for Extracting sub-tables from a Table\ncliques. For each maximal clique \ud835\udc36\ud835\udc56, the attributes corresponding\nto the nodes within it form a sub-table. The rationale behind using\nmaximal cliques is that each clique represents a subset of attributes\nwhere every attribute is significantly related to every other attribute\nwithin that subset, thus forming a highly cohesive and potentially\ntask-relevant sub-table.\nExample 1. As shown in Figure 6, in the overlap scenario, the\nattributes of the auxiliary table \ud835\udc471form the graph \ud835\udc3a\ud835\udc60\ud835\udc56\ud835\udc54. In this graph,\nthe nodes\ud835\udc361,\ud835\udc362, and\ud835\udc363are all interconnected, forming a maximal\nclique{\ud835\udc361,\ud835\udc362,\ud835\udc363}, which constitutes a sub-table \ud835\udc471,2.\nRemark. It is important to note that this strategy may result in sub-\ntables that are either disjoint or overlapping. As shown in Figure 5,\nDisjoint produces two disjoint sub-tables, while Overlap produces\ntwo overlapping sub-tables. This potential for disjointness and\noverlap reflects the complex and potentially multifaceted nature of\nattribute relationships relevant to the prediction task.\n5 GRAPH-BASED DATA AUGMENTATION\nFollowing the modeling of intra-table attribute relationships to cap-\nture semantic dependencies and partition tables into more focused\nsegments, as detailed in the previous section, we now turn our at-\ntention to the crucial task of modeling inter-table relationships. In\nthis section, we introduce a approach to construct a heterogeneous\ngraph that explicitly encodes the connections between tuples across\nthese partitioned tables and the base table. This enables us to select\ntask-relevant attributes and augment init tuples that span the entire\nrelational schema.\n5.1 Constructing the Inter-Table Graph\nWhile the partitioned sub-tables offer more focused representations\nof information within individual tables, the true predictive power\noften lies in the intricate relationships between entities residing\nacross these different tables. To effectively capture such inter-table\ndependencies, a graph-based learning emerges as a natural and\nintuitive choice. However, the construction of such a graph in\nour context presents two key challenges that necessitate careful\nconsideration:\n\u2022Original v.s. Splitted Tuples. How do we effectively link\nbase table tuples to relevant information distributed across thepartitioned auxiliary tables? Moreover, how are inter-table re-\nlationships accurately modeled when a single original tuple is\nrepresented by multiple sub-tuples, ensuring proper informa-\ntion aggregation?\n\u2022Supervision Isolation. Since only base table nodes are labeled\nand the graph structure is defined exclusively by join relation-\nships, semantically similar nodes are often disconnected in the\ntopology, which limits the propagation of supervision signals\nand hinders the learning of consistent representations.\nTo address these two key challenges, we propose a novel ap-\nproach for constructing a heterogeneous graph. The heterogeneous\ngraph, denoted byG=(V,E,X), is constructed as follows:\nNodes. Each tuple in the base table \ud835\udc470and each tuple within the\npartitioned sub-tables \ud835\udc47\ud835\udc56_1,...,\ud835\udc47\ud835\udc56_\ud835\udc57of the auxiliary tables \ud835\udc47\ud835\udc56con-\nstitutes a node inG. Thus, the set of nodes Vis the union of all\ntuples from the base tables and all sub-tables from the partitioned\nauxiliary tables.\nFeatures. Each node in the heterogeneous graph, representing a\ntuple, is associated with a feature vector derived from its constituent\nattribute values. Given the diverse nature of attributes in relational\ntables, which can be numerical, categorical, or textual, we employ\nmodality-specific encoders to generate meaningful embeddings\nfor each attribute. Following the idea in [ 14], we use a modality-\nspecific encoder to embed each attribute into embeddings. Once\neach attribute within a tuple is embedded, we concatenate these\nattribute embeddings to form the final feature vector for the node.\nEdges. We define two primary types of edges within our heteroge-\nneous graph, capturing both implicit relationships within the base\ntable and explicit, schema-defined connections across tables:\n\u2022Explicit Edges(Inter-Table Connections): Consider tuple \ud835\udc61\ud835\udc62from\none table and tuple \ud835\udc61\ud835\udc63from a different table. We add an edge\nbetween\ud835\udc61\ud835\udc62and\ud835\udc61\ud835\udc63if tuple\ud835\udc61\ud835\udc62and\ud835\udc61\ud835\udc63can be joined based on the\nprimary and foreign key relationship defined in the database\nschema. Moreover, if an original auxiliary table \ud835\udc47\ud835\udc56is partitioned\ninto sub-tables \ud835\udc47\ud835\udc561,...,\ud835\udc47\ud835\udc56\ud835\udc5a, a tuple\ud835\udc61from\ud835\udc47\ud835\udc56is correspondingly\nrepresented by sub-tuples \ud835\udc611\u2208\ud835\udc47\ud835\udc561,\ud835\udc612\u2208\ud835\udc47\ud835\udc562,...,\ud835\udc61\ud835\udc5a\u2208\ud835\udc47\ud835\udc56\ud835\udc5a. If\na tuple\ud835\udc61\u2032from another table can be joined with the original\ntuple\ud835\udc61, we create edges from \ud835\udc61\u2032to each of the sub-tuples of \ud835\udc61,\ni.e.,(\ud835\udc61\u2032,\ud835\udc611),...,(\ud835\udc61\u2032,\ud835\udc61\ud835\udc5a). This ensures that information from a\njoinable tuple is propagated to all the tuples from the partitioned\nsub-tables.\n\u2022Implicit Edges(within the Base Table): Tuples in the base table\nmight represent entities that are semantically similar or related\nin ways not directly encoded by joins. For instance, two cus-\ntomer records might have very similar purchasing histories or\ndemographic profiles, even if there isn\u2019t a direct foreign key\nrelationship linking them. To capture such semantic similarity,\nwe create edges between tuples in the base table based on the\nsimilarity between their corresponding feature vectors. Specifi-\ncally, for any two tuples from the base table \ud835\udc470, we create an\nedge between them if the similarity between their features is\nabove a predefined threshold \ud835\udf03. Alternatively, we can adopt a\nmore selective approach by connecting each tuple to its top- \ud835\udc3e\nmost similar tuples within the base table. By limiting the num-\nber of implicit connections per node, we can create a sparser\n\n--- Page 8 ---\nLianpeng Qiao, Ziqi Cao, Kaiyu Feng, Ye Yuan, and Guoren Wang\ngraph compared to a threshold-based approach. This can lead\nto more efficient message passing during GNN training and\npotentially reduce computational overhead.\n5.2 Augmenting Table\nThe constructed heterogeneous graph serves as a comprehensive\nrepresentation of the intricate relationships among tuples span-\nning the base table and all partitioned auxiliary tables. However,\nto effectively leverage this complex structure for both feature se-\nlection and feature enhancement, we require a mechanism capable\nof performing information propagation and aggregation over the\ngraph, thereby learning fine-grained edge weights that capture\nthese diverse relational patterns.\nGNNs perform message passing to allow each node to gather\ninformation from its neighbors, weighted by the edge connections,\nand iteratively refine its own representation. This process enables\nthe model to learn high-order relationships and dependencies that\nare implicitly encoded within the graph structure, ultimately yield-\ning tuple embeddings that are informed by the broader relational\ncontext and tailored for accurate prediction of the target column in\nthe base table.\nConsider a heterogeneous graph \ud835\udc3a=(V,E,Tv,Te), where Vis\nthe set of nodes, Eis the set of edges, Tvis the set of node types,\nandTeis the set of edge types.\nAggregation Function For a target node \ud835\udc63\ud835\udc56from the base table\nand an edge type \ud835\udc61\u2208\ud835\udc47\ud835\udc52, messages are generated from its neighbors\n\ud835\udc41\ud835\udc63\ud835\udc56based on different edge types:\n\ud835\udc5a\ud835\udc61\n\ud835\udc56=\u2211\ufe01\n\ud835\udc57\u2208\ud835\udc41\ud835\udc61\ud835\udc63\ud835\udc56\ud835\udc40(\u210e\ud835\udc57,\u210e\ud835\udc52\ud835\udc57\ud835\udc56,\u210e\ud835\udc56) (7)\nHere,\ud835\udc40is the message generation function, \u210e\ud835\udc57is the feature of\nneighbor node \ud835\udc57,\u210e\ud835\udc52\ud835\udc57\ud835\udc56is the edge feature, \u210e\ud835\udc56is the feature of node \ud835\udc56,\nand\ud835\udc41\ud835\udc61\ud835\udc63\ud835\udc56denotes the set of neighbor nodes connected to node \ud835\udc56via\nedges of type \ud835\udc61.\nMessage Passing The messages \ud835\udc5a\ud835\udc61\n\ud835\udc56generated from each edge type\n\ud835\udc61\u2208\ud835\udc47\ud835\udc52are integrated:\n\u210e(\ud835\udc59+1)\n\ud835\udc56=\ud835\udc48(\u210e(\ud835\udc59)\n\ud835\udc56,{\ud835\udc5a\ud835\udc61\n\ud835\udc56|\ud835\udc61\u2208\ud835\udc47\ud835\udc52}) (8)\nHere,\ud835\udc48is the message updating function, \u210e(\ud835\udc59)\n\ud835\udc56denotes the feature\nof node\ud835\udc56at layer\ud835\udc59,\u210e(\ud835\udc59+1)\n\ud835\udc56denotes the feature of node \ud835\udc56at layer\ud835\udc59+1,\nand{\ud835\udc5a\ud835\udc61\n\ud835\udc56|\ud835\udc61\u2208\ud835\udc47\ud835\udc52}is the set of all messages received from different\nedge types.\nPrediction The total loss is computed by aggregating the prediction\nlosses over all labeled nodes:\nL\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59=\ud835\udc41\u2211\ufe01\n\ud835\udc56=1L(\ud835\udc66\ud835\udc56,\ud835\udc53\ud835\udc3a(\ud835\udc65\ud835\udc56)) (9)\nwhere\ud835\udc41is the number of labeled nodes, \ud835\udc65\ud835\udc56denotes the features\nof node\ud835\udc56,\ud835\udc66\ud835\udc56is its ground-truth label, Lis the task-specific loss\nfunction, and \ud835\udc53\ud835\udc3ais a heterogeneous graph neural network.\nOur research aims to use a GNN to develop advanced repre-\nsentations for tuples in the base table by integrating vital data\nfrom auxiliary tables. Given that labels exist solely for the base\ntable nodes, we employ the GNN to spread label data, boostingthe base table node representations\u2019 accuracy and predictive abil-\nity. Specifically, by refining the graph\u2019s edge weights, we evaluate\neach edge\u2019s significance to improve predictions, assuming more\nimportant edges receive higher weights during training. The cen-\ntral task is to boost the tuple representations\u2019 predictive accuracy\nfrom the base table. We test these enriched representations in real-\nworld prediction contexts to pinpoint the most valuable features.\nThis issue is defined as enhancing prediction efficacy according to\nthe formula 9. This strategy results in selecting features linked to\nhigh-weight edges, marking them as crucial for the prediction task.\nThe enhanced representations from the base table not only reflect\nexplicit inter-table relationships but also capture hidden semantic\nconnections, thereby achieving superior predictive accuracy amidst\nintricate patterns across diverse tables.\n6 EXPERIMENTAL STUDY\nIn this section, we focus on answering the following questions:\nRQ1: How does ReCoGNN perform compared to various baseline?\nRQ2: How do key parameters affect the performance of ReCoGNN ?\nRQ3: How do different components contribute to the system\u2019s\nperformance?\n6.1 Experimental Settings\nDatasets. We use 10 datasets from various domains to evaluate our\nReCoGNN . As shown in Table 1, we give the basic information on\nthe datasets. Among the datasets, six of them are for classification\ntasks and the other four datasets for regression tasks. Now we\npresent the detailed information of these datasets.\n\u2022Olist [2] is derived from the Brazilian e-commerce platform\nOlist and is utilized for predicting customer satisfaction. A\ncustomer is deemed satisfied if their review_score is at least 3.\n\u2022MovieLens [6] is compiled by GroupLens Research from the\nMovieLens platform aiming to forecast user gender.\n\u2022Loyal [3] aims to enhance credit card holders\u2019 personalized rec-\nommendations by employing machine learning for predicting\ncustomer loyalty.\n\u2022PED[7] from DonorsChoose.org is an online charity aiding K-12\nschools through project-based donations. The task is to identify\nexceptionally exciting projects.\n\u2022Event [4] includes user actions, event metadata, and other infor-\nmation, in order to predict which events users will be interested\nin.\n\u2022Event-Not [4] aligns with the event dataset but focuses on\npredicting the events that are not of user interest.\n\u2022F1[5] contains comprehensive historical data from all Formula\nOne seasons since 1950. The regression task involves predicting\nthe average position order of a driver.\n\u2022IMDB [27] predicts the scores of movies and the dataset includes\nindividual characteristics of movies.\n\u2022Restbase [1] comprises restaurant data, framing the prediction\nof customer review details as a regression task.\n\u2022Bio[8] dataset is utilized for a regression task aimed at esti-\nmating \"the molecule\u2019s bioactivity\".\nBaseline. Our methodology is evaluated against various solution.\n(1)Base retains only the native features of the base table.\n(2)Allsimply joins all auxiliary tables with the base table.\n\n--- Page 9 ---\nGraph-Based Feature Augmentation for Predictive Tasks on Relational Datasets\nTable 1: Statistics of Datasets\nDataset Tables Features Rows Task\nOlist 4 25 401K Class.\nMovieLens 3 29 1M Class.\nLoyal 4 58 118K Class.\nPED 3 50 18K Class.\nEvent 5 129 46K Class.\nEvent-Not 5 129 46K Class.\nF1 9 62 133K Reg.\nIMDB 7 45 203K Reg.\nRestbase 3 9 28K Reg.\nBio 5 11 21K Reg.\n(3)Random randomly selects \ud835\udc3efeatures from auxiliary tables.\n(4)Mutual Information ( MI)is the filter-based feature selection\nmethod that evaluates the importance of features through dif-\nferent mechanisms and selects the top- \ud835\udc3efeature subset.\n(5)Backward Elimination ( BE)[19] is a wrapper-based feature\naugmentation method that starts by joining all auxiliary tables\nand then iteratively removes the features that most degrade\nmodel performance. The final subset is achieved by step-by-step\nexclusion of the least useful features.\n(6)XGBoost ,LightGBM and Random Forest ( RF)[9,10,24] are\nembedded feature selection methods that combine the advan-\ntages of filter methods and wrapper methods by automatically\nperforming feature selection during the model training process.\n(7)ARDA [11] combines the methods of building the core set and se-\nlecting random injection features, effectively screening out fea-\ntures that contribute to improving model accuracy while avoid-\ning the introduction of noisy features. As the ARDA-NoText\nalgorithm is incapable of directly handling text-type data, we\nimplement two approaches: either converting text attributes to\nhigh-dimensional vectors ( ARDA ) or eliminating them entirely\n(ARDA-NoText ).\n(8)LEVA [44] is an end-to-end system that constructs relational\nembeddings to represent relational data as vectors, which are\nthen used to efficiently describe the base table. It constructs a\ngraph to represent relational data and embeds this graph into\nhigh-dimensional space.\nEvaluation Metrics. Model performance evaluation is conducted\nusing the following methodology: For classification tasks, pri-\nmary metrics include AUC-ROC andAccuracy , complemented by F1\nScore andAverage Precision for a detailed performance analysis\nin parametric sensitivity studies. In regression tasks, evaluation\nrelies on Mean Squared Error ( MAE) and Mean Absolute Error ( MSE).\nIn this research, we applied random sampling to the initial\ndatasets, namely Olist ,Event (Event-Not ),PED, and Loyal , be-\ncause of their large size.\nImplementation. In our experiment, we chose Python as the\nprogramming language. For the GNN part, we used the PyTorch-\nGeometric library, specifically adopting the GraphSAGE[ 20] and\nGAT[ 38] models. To encode the relational tables, we utilized the\nPyTorch Frame [ 21] library. Furthermore, training of the XGBoost\nandLightGBM baseline models also directly used the interfaces pro-\nvided by the PyTorch Frame library. Unless otherwise specified, thedefault threshold \u2113for extracting important attribute relationships\nis set to 0.8. Each experimental trial was set with an 18-hour time\nlimit, at which point the process would automatically conclude\nwithout achieving the results.\n6.2 Comparison with Baselines\nHere, we evaluate the performance of the ReCoGNN model against\nother baselines, focusing on classification and regression tasks.\nClassification. As illustrated in Table 2, we assess a variety of base-\nline methods across multiple datasets and utilizing different models.\nWe use Accuracy andAUC-ROC metrics to evaluate each dataset\nacross different models. Accuracy provides a straightforward and\neasily understandable performance metric, while AUC-ROC comple-\nments it by being effective in situations with imbalanced data. Table\n2 shows the effectiveness of different methods on the six datasets.\nthe performance of the base methods, including Base Table, Big Ta-\nble, and Random, was inferior to that of embedding-based methods\n(including XGBoost ,LightGBM , and RF) as well as the ReCoGNN\nmethod. The experimental results indicate that using only the Base\nTable generally yielded poor results, and in the Olist dataset, the\nAllmethod performed even worse than the Base Table, with the\nAUC-ROC significantly dropping from 0.8296 to 0.5616. This is be-\ncause, although the Allmethod added more features through join\noperations, the unfiltered features introduced noise. Meanwhile,\nthe all-table join operation might also lead to data drift issues. The\nperformance of the Random method was unstable: in the Loyal\ndataset, its metrics were lower than those of the Base andAll,\nwhile in the Event-Not dataset, it performed better than the Base .\nThis indicates that randomly selecting features might yield useful\nfeatures or might introduce noise, thus systematic feature selection\nis necessary.\nEmbedding-based methods ( XGBoost ,LightGBM , and RF) per-\nformed the best among all baselines. For example, in the Olist\ndataset, LightGBM achieved the best results in both Accuracy and\nAUC-ROC metrics. The advantage of these methods lies in their abil-\nity to dynamically evaluate feature importance during training\nand gradually optimize the feature subset. However, the down-\nside is the high computational cost: XGBoost (100 epochs) took\n35,899.234 seconds, LightGBM (100 epochs) took 225,340.247 sec-\nonds, and ReCoGNN (200 epochs) took 9,284.666 seconds.\nThe mutual information method based on filtering performed\nworse than the embedding methods in all experiments. For instance,\nin the Loyal dataset, the AUC-ROC of the mutual information method\nwas only 0.5088, significantly lower than that of XGBoost (0.5575),\nLightGBM (0.5339), and RF(0.5723). This performance gap stems\nfrom two main factors: firstly, filtering methods evaluate features\nsolely based on univariate statistics, completely ignoring the inter-\nactions among features; secondly, their feature selection process is\nindependent of model training and does not obtain real-time feed-\nback on prediction performance. It is particularly noteworthy that\ntraditional filtering methods lack effective mechanisms to handle\ntext-type features (which need to be encoded into 300-dimensional\nvectors), further limiting their performance.\nThe BEmethod based on wrappers, although iteratively refer-\nencing model feedback to select features, neither outperformed the\nmutual information method nor reached the level of embedding\n\n--- Page 10 ---\nLianpeng Qiao, Ziqi Cao, Kaiyu Feng, Ye Yuan, and Guoren Wang\nTable 2: Results of the Classification Task (bold: the best; underline: the second best; Acc is Accuracy ; AUC is AUC-ROC )\nMethodOlist MovieLens Loyal PED Event Event-Not\nAcc AUC-ROC Acc AUC-ROC Acc AUC-ROC Acc AUC-ROC Acc AUC-ROC Acc AUC-ROC\nBase 0.852 0.8296 0.7036 0.5213 0.5424 0.5111 0.515 0.5257 0.7323 0.5022 0.948 0.5443\nAll 0.8339 0.5616 0.7517 0.5001 0.5353 0.5146 0.6069 0.5842 0.7456 0.5701 0.9757 0.5344\nRandom 0.8335 0.5113 0.7216 0.5547 0.5203 0.5034 0.5344 0.5437 0.7396 0.6526 0.9736 0.5500\nMI 0.8335 0.5506 0.7216 0.5281 0.5203 0.5088 0.6069 0.5326 0.7505 0.5001 0.9729 0.5165\nBE 0.8527 0.5001 0.7525 0.5001 0.5421 0.5001 0.5336 0.5007 - - - -\nXGBoost 0.9068 0.8852 0.7011 0.5988 0.5421 0.5575 0.6625 0.5337 0.7352 0.6305 0.9729 0.7691\nLightGBM 0.9130 0.8984 0.7051 0.6245 0.5421 0.5339 0.6218 0.5834 0.7352 0.6068 0.9729 0.6966\nRF 0.8516 0.8677 0.7012 0.6046 0.5355 0.5723 0.6039 0.5041 0.7505 0.6221 0.9727 0.5729\nARDA 0.8526 0.5003 0.7011 0.5048 0.5421 0.5078 0.5276 0.5002 0.7395 0.5023 0.9729 0.5020\nARDA-NoText 0.8526 0.5736 0.7543 0.5066 0.5649 0.5855 0.5461 0.5007 0.7352 0.5173 0.9332 0.5049\nLEVA 0.8595 0.5186 0.7152 0.5319 0.52 0.5105 0.5525 0.5528 0.7396 0.5 0.9652 0.5015\nReCoGNN 0.9084 0.8855 0.7980 0.8432 0.5825 0.6397 0.685 0.7462 0.7516 0.6918 0.9603 0.9233\nTable 3: Results of the Regress Task (bold: the best; underline: the second best)\nMethodF1 IMDB Restbase Bio\nMAE MSE MAE MSE MAE MSE MAE MSE\nBase 489.878 248807.529 6.935 50.563 0.445 0.341 0.925 1.316\nAll 102.814 68337.160 1.539 4.578 0.442 0.345 1.048 1.732\nRandom 306.206 113842.427 1.546 3.975 0.406 0.323 1.462 3.449\nMI 6.734 66.279 1.224 2.406 0.401 0.278 1.80 4.144\nBE 15.048 350.338 1.247 2.453 0.411 0.265 3.409 12.875\nXGBoost 3.129 13.913 - - 0.342 0.306 1.241 2.205\nLightGBM 3.109 13.679 1.199 2.317 0.336 0.320 1.114 1.177\nRF 2.788 14.334 - - 0.416 0.239 1.213 2.182\nARDA 23.548 1347.496 2.123 5.623 0.407 0.269 3.4 13.457\nARDA-NoText 9.188 103.613 1.246 1.552 0.409 0.268 5.241 31.73\nLEVA 4.864 40.4845 1.339 3.017 0.302 0.247 1.28 3.727\nReCoGNN 0.912 3.227 1.113 2.311 0.275 0.230 0.521 0.5826\nmethods. This method has two main deficiencies: first, constrained\nby a greedy search strategy, it has difficulty fully exploring the\nfeature space and is prone to local optima; second, similar to fil-\ntering methods, it faces technical bottlenecks when handling high-\ndimensional text features. These limitations prevent the BEmethod\nfrom discovering the optimal feature combination, ultimately af-\nfecting overall prediction performance.\nThe performance of ARDA and ARDA-NoText was inferior to\nembedding-based methods and our method in all datasets. This\nis attributable to three main factors: first, the original ARDA-NoText\nalgorithm does not support text feature processing, and the mod-\nified ARDA-NoText version in the baseline experiments sacrificed\nsome performance for this; second, ARDA-NoText avoided process-\ning issues by removing text features but led to the loss of effective\nfeature information; finally, although the Leva method achieved\nbetter Accuracy values than other baselines in most datasets due to\nits graph structure design, its performance improvement was lim-\nited by the inability to fully explore potential relationships among\nfeatures.\nOur proposed ReCoGNN solution outperformed all baseline\nmethods in terms of Accuracy andAUC-ROC metrics in the Movie,\nLoyal, and PED datasets, and also demonstrated competitive advan-\ntages in other datasets. The technical advantages of ReCoGNN lie intwo aspects: 1) deeply exploring the potential relationships among\nfeatures within the same table and finely splitting auxiliary tables;\n2) accurately modeling join relationships among tables through\na weighted heterogeneous graph structure, thus achieving more\nprecise feature enrichment. These mechanisms collectively ensure\nthe outstanding performance of the ReCoGNN method.\nRegression. Using the same baselines as the classification experi-\nments to construct regression tasks, Table 3 presents the experimen-\ntal results, with performance evaluation using MAEandMSE. The\ntrend of results is consistent with the classification experiments.\nThe basic methods, including Base ,All, and Random , were inferior\nto the embedding-based methods (including XGBoost ,LightGBM ,\nandRF) as well as the ReCoGNN method. Moreover, embedding-\nbased methods outperformed filter-based (mutual information) and\nwrapper-based (backward elimination) methods across all datasets.\nLEVA andARDA-NoText were superior to most baseline methods\nbut inferior to our method.\nSummary. To achieve better predictive performance, the key fac-\ntors are: 1) adding features to the base table; and 2) ensuring that\nthe added features are useful and do not introduce noise. By ex-\nploring the potential relationships among features within the same\ntable and using weighted heterogeneous graphs to capture table\n\n--- Page 11 ---\nGraph-Based Feature Augmentation for Predictive Tasks on Relational Datasets\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.5000.5750.6500.7250.8000.875 Effectiveness\n\u2113Average Precision\nAccuracyF1 Score\nROC-AUC\nFigure 6: Performance of Different \u2113onMovieLens Dataset\nTable 4: Performance of Different Graph Discovery Algo-\nrithms on MovieLens Datasets (bold: the best)\nMethod\u2113=0.5 \u2113=0.8\nComplete GN Complete GN\nAverage Precision 0.7257 0.7226 0.6988 0.6262\nAccuracy 0.7980 0.7781 0.7815 0.7566\nF1 Score 0.6738 0.665 0.6118 0.5067\nAUC-ROC 0.8432 0.8274 0.8243 0.7689\nstructure, the ReCoGNN method ensures superior performance\ncompared to other baselines.\n6.3 Sensitivity of ReCoGNN\nWe explore the impact of varying filtering parameters \u2113on model\nperformance within the MovieLens dataset, alongside the effect of \u2113\nvariations. As illustrated in Fig. 6, different \u2113values influence model\nperformance, depicted through a range of matrices. It is evident\nacross these matrices that an increase in \u2113initially strengthens and\nsubsequently weakens the model\u2019s predictive capability. This occurs\nbecause, at a low \u2113, the relational dynamics between features are\ninadequately captured; as \u2113increases, it begins to adeptly capture\nfeature relationships within the same table. However, if \u2113grows\ntoo large, it may excessively capture data, thereby reducing model\nefficacy. Notably, when \u2113ranges from 0.1 to 0.4 and 0.5 to 0.7, all\nmetrics are consistent due to the similar relationships captured by\nthese parameter values.\nAs shown in Table 4, we can see that different graph discovery\nalgorithms affect the performance of ReCoGNN on the MovieLens\ndataset. We compared two graph discovery algorithms, one is Com-\nplete Graph, and the other is Girvan-Newman(GN). The Girvan-\nNewman algorithm achieves community detection by continuously\nremoving edges with high centrality, causing the network to grad-\nually split into multiple tightly interconnected communities. At\n\u2113=0.5 and 0.8, the Complete Graph algorithm performs better\nthan the Girvan-Newman algorithm, mainly because the Complete\nGraph algorithm is stricter in cluster discovery, discovering strong\nrelationships between features and thereby reducing noise.6.4 Ablation Experiments\n6.4.1 Attribute Relationship Mining. Attribute Relationship Min-\ning: In the preceding discussion, we emphasized the importance\nof mining potential relationships between attributes in relational\ntable data. Attributes with semantic or co-dependency relationships\nshould be grouped together to preserve such information. Splitting\ntuples into node groups based on these relationships allows the\ngraph to encode group-specific patterns. However, overly aggres-\nsive splitting may fragment coherent attributes, while insufficient\nmining risks merging unrelated attributes, both of which could\ndegrade the performance of downstream tasks. We designed an ab-\nlation study to evaluate the impact of attribute relationship mining.\nThree configurations were compared:\n\u2022Graph-based mining: The method mentioned in Chapter 4,\nwhich detects correlations between attributes by statistically\nanalyzing edge weight coefficients k.\n\u2022Random grouping: Attributes are split into random node pairs\nwithout relationship analysis.\n\u2022No mining: Each attribute is treated as an independent node.\nUsing Accuracy andAUC-ROC as evaluation metrics, we conducted\nexperiments on the PEDandLoyal classification datasets. As shown\nin Figure 7, the accuracy of the feature latent relationship mining\nmodel on the relationship table is significantly better than that\nof randomly combined feature relationships and without feature\nrelationship mining.\n6.4.2 Edge Weights. In section 5, we discussed how to use weight\ngraphs to enrich and predict the features of each tuple in the base\ntable. To examine the impact of edge weights on predictive perfor-\nmance, we conduct an ablation study by modifying the graph\u2019s edge\nproperties. In this study, we compare the results of models with\nand without edge weights to assess their influence on the overall\naccuracy. As shown in Fig. 8, in the PEDandLoyal datasets, the\nprediction accuracy on the weighted graphs surpasses that of the\nmethods without introduced weights; the same significant effect\nis observed in regression data sets. This is because GNN can learn\nedge weights, effectively mitigating the impact of noisy features on\nthe final prediction during the message-passing process, thereby\nenhancing the overall performance of the model.\n6.4.3 Graph Construction Schema. To enhance the prediction accu-\nracy by using relational data, we convert multi-table datasets into\ngraphs. The primary table, known as the base table, contains nodes\nwith labels, while the other tables provide auxiliary information to\nenrich the feature space of the base table. The experiment focuses\non the impact of retaining or removing similarity-based edges on\nprediction performance. As depicted in Fig. 2, we establish two\ntypes of edges in our graph construction: (1) join relations between\ntables, and (2) similarity-based edges within the base table. For\nthe similarity-based edges, each row in the base table is encoded\ninto an embedding, and edges are added between tuples with high\nsimilarity scores. To evaluate the impact of these similarity edges,\nwe conduct two sets of experiments: one with similarity edges in-\ncluded and another with them removed. From the results shown\nin Fig 8, it is evident that retaining similarity-based edges in the\ngraph significantly boosts the predictive accuracy or enhances the\nmodel\u2019s prediction precision. This is because introducing similarity\n\n--- Page 12 ---\nLianpeng Qiao, Ziqi Cao, Kaiyu Feng, Ye Yuan, and Guoren Wang\nAccuracy ROC-AUC0.600.650.700.750.80 Effectiveness\n(a) PEDAccuracy ROC-AUC0.500.550.600.650.70 Effectiveness\n(b) LoyalReCoGNN Random No-mining\nFigure 7: Ablation Studies of Attributes Relationship Discov-\nery\nAccuracy ROC-AUC0.600.650.700.750.80 Effectiveness\n(a) PEDAccuracy ROC-AUC0.500.550.600.650.70 Effectiveness\n(b) Loyal\nMAE MSE0.500.700.901.101.30 Value\n(c) BioMAE MSE0.51.52.53.54.55.5 Value\n(d) F1ReCoGNN No-Weight No-Edge\nFigure 8: Ablation Studies on Edge Weights and Added Simi-\nlarity Edges\nedges can enhance the flow of information between nodes from\nthe base table. More efficient information transmission can prompt\nthe model to learn more optimized node embeddings. This greatly\nbenefits downstream tasks such as classification and regression.\n7 RELATED WORK\nFeature Augmentation Several notable works have addressed fea-\nture augmentation from relational tables. Early studies like [ 26,35]\navoided unnecessary joins by leveraging foreign key constraints.\nLater, ARDA [ 11] proposed an end-to-end automated system with\nheuristic feature selection, while AutoFeature [ 31] introduced re-\ninforcement learning for exploration-exploitation trade-offs. Aut-\nofeat [ 22] extended this by discovering transitive features through\nmulti-hop joins, albeit with increased computational overhead. Fur-\nther improvements include METAM\u2019s [ 15] goal-oriented discovery\nframework and FEATPILOT\u2019s [ 30] efficient multi-hop augmenta-\ntion via clustering and LSTM-based prediction. Alternatively, [ 39]\nadopted coreset-based selection to bypass table materialization\nwhile preserving accuracy.\nModeling Attribute Relationships in Tabular Data Attribute\ninteraction methods are classified into Factorization Machines (FM)\n[33], their enhancements, deep learning-based models, and recentGNN-based approaches. FM traditionally handle second-order fea-\nture interactions using vector inner products. Enhanced versions\nlike Field-aware Factorization Machines [ 23] and Attentional Factor-\nization Machines [ 40] improve interaction modeling. Deep learning\napproaches, such as DeepFM [ 18] and xDeepFM [ 29], merge FM\nand deep networks to capture explicit and implicit high-order in-\nteractions. GNNs excel at modeling complex feature relationships;\nFi-GNN [ 28], for instance, treats features as nodes and interactions\nas edges for versatile attribute representation.\nGNN for Tabular Data In recent research, Graph Neural Net-\nwork (GNN) models have become prominent for processing rela-\ntional data, expertly crafted to handle graph-structured information.\nThese models iteratively refine node embeddings by integrating\ndata from both the nodes and their neighbors, using these enhanced\nrepresentations for predictions. Key contributions in linking rela-\ntional data with GNNs are from Schlichtkrull et al. [ 34], Cvitkovic et\nal. [13], and Zahradn\u00edk et al. [ 42]. GNN models can be categorized\ninto inductive and transductive types based on their capability to\nmanage unseen nodes. Inductive GNNs, such as GraphSAGE [ 20],\ncan generalize to new nodes or graphs. In contrast, transductive\nGNNs like GCN [ 25] and GIN [ 41] optimize node representations\nusing the full graph but cannot adapt to new graphs. A hybrid\nmodel is GAT [ 38], which supports both approaches. Our model\nemploys the inductive strategy, allowing it to accommodate newly\nintroduced samples in future scenarios.\n8 CONCLUSION\nIn this paper,we introduce ReCoGNN , an autonomous feature aug-\nmentation framework. The framework employs a two-stage pro-\ngressive column relationships discovery mechanism:(1) ReCoGNN\nperforms parallelized modeling of latent inter-column dependencies\nwithin each auxiliary table, followed by partitioning each auxiliary\ntable into semantically coherent segments with enhanced attribute\ncorrelations. (2) In the next stage, we construct a weighted heteroge-\nneous graph incorporating both the base table and segmented data,\nenabling the GNN\u2019s message passing to perform single-pass fea-\nture selection and base table feature augmentation. We conducted\ncomprehensive experiments on real-world datasets to evaluate the\nperformance of ReCoGNN against other baseline methods. The\nresults demonstrate that ReCoGNN consistently outperforms the\nbaselines and exhibits superior performance.\n\n--- Page 13 ---\nGraph-Based Feature Augmentation for Predictive Tasks on Relational Datasets\nREFERENCES\n[1] [n.d.]. https://sourceforge.net/projects/proper/files/Datasets/raw/\n[2] [n.d.]. Brazilian E-Commerce Public Dataset by Olist. https://www.kaggle.com/\ndatasets/olistbr/brazilian-ecommerce/\n[3] [n.d.]. Elo Merchant Category Recommendation. https://www.kaggle.com/c/elo-\nmerchant-category-recommendation\n[4][n.d.]. Event Recommendation Engine Challenge. https://www.kaggle.com/\ncompetitions/event-recommendation-engine-challenge\n[5] [n.d.]. Formula One. https://ergast.com/mrd/\n[6] [n.d.]. MovieLens 1M Dataset. https://grouplens.org/datasets/movielens/1m/\n[7] [n.d.]. Predicting Excitement at DonorsChoose. https://www.kaggle.com/c/kdd-\ncup-2014-predicting-excitement-at-donors-choose\n[8]Hendrik Blockeel, Sa\u0161o D\u017eeroski, Boris Kompare, Stefan Kramer, Bernhard\nPfahringer, and WIM VAN LAER. 2004. Experiments in predicting biodegrad-\nability. Applied Artificial Intelligence 18, 2 (2004), 157\u2013181.\n[9] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5\u201332.\n[10] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.\nInProceedings of the 22nd acm sigkdd international conference on knowledge\ndiscovery and data mining . 785\u2013794.\n[11] Nadiia Chepurko, Ryan Marcus, Emanuel Zgraggen, Raul Castro Fernandez, Tim\nKraska, and David Karger. 2020. ARDA: automatic relational data augmentation\nfor machine learning. arXiv preprint arXiv:2003.09758 (2020).\n[12] Jillian M Clements, Di Xu, Nooshin Yousefi, and Dmitry Efimov. 2020. Sequential\ndeep learning for credit risk monitoring with tabular financial data. arXiv preprint\narXiv:2012.15330 (2020).\n[13] Milan Cvitkovic. 2020. Supervised learning on relational databases with graph\nneural networks. arXiv preprint arXiv:2002.02046 (2020).\n[14] Matthias Fey, Weihua Hu, Kexin Huang, Jan Eric Lenssen, Rishabh Ranjan,\nJoshua Robinson, Rex Ying, Jiaxuan You, and Jure Leskovec. 2023. Relational\ndeep learning: Graph representation learning on relational databases. arXiv\npreprint arXiv:2312.04615 (2023).\n[15] Sainyam Galhotra, Yue Gong, and Raul Castro Fernandez. 2023. Metam: Goal-\noriented data discovery. In 2023 IEEE 39th International Conference on Data\nEngineering (ICDE) . IEEE, 2780\u20132793.\n[16] Yury Gorishniy, Ivan Rubachev, and Artem Babenko. 2022. On embeddings for\nnumerical features in tabular deep learning. Advances in Neural Information\nProcessing Systems 35 (2022), 24991\u201325004.\n[17] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. 2021.\nRevisiting deep learning models for tabular data. Advances in Neural Information\nProcessing Systems 34 (2021), 18932\u201318943.\n[18] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\nDeepFM: a factorization-machine based neural network for CTR prediction.\narXiv preprint arXiv:1703.04247 (2017).\n[19] Isabelle Guyon and Andr\u00e9 Elisseeff. 2003. An introduction to variable and feature\nselection. Journal of machine learning research 3, Mar (2003), 1157\u20131182.\n[20] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation\nlearning on large graphs. Advances in neural information processing systems 30\n(2017).\n[21] Weihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey. 2024. PyTorch Frame: A Modular Framework\nfor Multi-Modal Tabular Learning. arXiv preprint arXiv:2404.00776 (2024).\n[22] Andra Ionescu, Kiril Vasilev, Florena Buse, Rihan Hai, and Asterios Katsifodimos.\n2024. AutoFeat: Transitive Feature Discovery over Join Paths. In 2024 IEEE 40th\nInternational Conference on Data Engineering (ICDE) . IEEE, 1861\u20131873.\n[23] Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-\naware factorization machines for CTR prediction. In Proceedings of the 10th ACM\nconference on recommender systems . 43\u201350.\n[24] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,\nQiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boostingdecision tree. Advances in neural information processing systems 30 (2017).\n[25] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with\ngraph convolutional networks. arXiv preprint arXiv:1609.02907 (2016).\n[26] Arun Kumar, Jeffrey Naughton, Jignesh M Patel, and Xiaojin Zhu. 2016. To join\nor not to join? thinking twice about joins before feature selection. In Proceedings\nof the 2016 International Conference on Management of Data . 19\u201334.\n[27] Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper, and\nThomas Neumann. 2015. How good are query optimizers, really? Proceedings of\nthe VLDB Endowment 9, 3 (2015), 204\u2013215.\n[28] Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-gnn:\nModeling feature interactions via graph neural networks for ctr prediction. In\nProceedings of the 28th ACM international conference on information and knowledge\nmanagement . 539\u2013548.\n[29] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and\nGuangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature in-\nteractions for recommender systems. In Proceedings of the 24th ACM SIGKDD\ninternational conference on knowledge discovery & data mining . 1754\u20131763.\n[30] Jiaming Liang, Chuan Lei, Xiao Qin, Jiani Zhang, Asterios Katsifodimos, Christos\nFaloutsos, and Huzefa Rangwala. 2025. FeatPilot: Automatic feature augmenta-\ntion on tabular data. (2025).\n[31] Jiabin Liu, Chengliang Chai, Yuyu Luo, Yin Lou, Jianhua Feng, and Nan Tang.\n2022. Feature augmentation with reinforcement learning. In 2022 IEEE 38th\nInternational Conference on Data Engineering (ICDE) . IEEE, 3360\u20133372.\n[32] N Reimers. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-\nNetworks. arXiv preprint arXiv:1908.10084 (2019).\n[33] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International conference\non data mining . IEEE, 995\u20131000.\n[34] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan\nTitov, and Max Welling. 2018. Modeling relational data with graph convolu-\ntional networks. In The semantic web: 15th international conference, ESWC 2018,\nHeraklion, Crete, Greece, June 3\u20137, 2018, proceedings 15 . Springer, 593\u2013607.\n[35] Vraj Shah, Arun Kumar, and Xiaojin Zhu. 2017. Are key-foreign key joins safe\nto avoid when learning high-capacity classifiers? arXiv preprint arXiv:1704.00485\n(2017).\n[36] Carl Van Walraven, Irfan A Dhalla, Chaim Bell, Edward Etchells, Ian G Stiell,\nKelly Zarnke, Peter C Austin, and Alan J Forster. 2010. Derivation and validation\nof an index to predict early death or unplanned readmission after discharge from\nhospital to the community. Cmaj 182, 6 (2010), 551\u2013557.\n[37] A Vaswani. 2017. Attention is all you need. Advances in Neural Information\nProcessing Systems (2017).\n[38] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\narXiv:1710.10903 (2017).\n[39] Jiayi Wang, Chengliang Chai, Nan Tang, Jiabin Liu, and Guoliang Li. 2022. Core-\nsets over multiple tables for feature-rich and data-efficient machine learning.\nProceedings of the VLDB Endowment 16, 1 (2022), 64\u201376.\n[40] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.\n2017. Attentional factorization machines: Learning the weight of feature interac-\ntions via attention networks. arXiv preprint arXiv:1708.04617 (2017).\n[41] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful\nare graph neural networks? arXiv preprint arXiv:1810.00826 (2018).\n[42] Luk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr. 2023. A deep learning blueprint\nfor relational databases. In NeurIPS 2023 Second Table Representation Learning\nWorkshop .\n[43] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based recom-\nmender system: A survey and new perspectives. ACM computing surveys (CSUR)\n52, 1 (2019), 1\u201338.\n[44] Zixuan Zhao and Raul Castro Fernandez. 2022. Leva: Boosting machine learning\nperformance with relational embedding data augmentation. In Proceedings of the\n2022 International Conference on Management of Data . 1504\u20131517.",
  "project_dir": "artifacts/projects/enhanced_cs.LG_2508.20986v1_Graph_Based_Feature_Augmentation_for_Predictive_Ta",
  "communication_dir": "artifacts/projects/enhanced_cs.LG_2508.20986v1_Graph_Based_Feature_Augmentation_for_Predictive_Ta/.agent_comm",
  "assigned_at": "2025-08-31T20:59:52.584235",
  "status": "assigned"
}